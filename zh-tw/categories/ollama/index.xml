<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on Ricky</title><link>https://linzeyan.github.io/zh-tw/categories/ollama/</link><description>Recent content in Ollama on Ricky</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><lastBuildDate>Thu, 25 Apr 2024 10:14:00 +0800</lastBuildDate><atom:link href="https://linzeyan.github.io/zh-tw/categories/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Run llama3</title><link>https://linzeyan.github.io/zh-tw/posts/20240425ollama/</link><pubDate>Thu, 25 Apr 2024 10:14:00 +0800</pubDate><guid>https://linzeyan.github.io/zh-tw/posts/20240425ollama/</guid><description>ollama Instructions Docker-compose version: &amp;#34;3.8&amp;#34; services: ollama: image: ollama/ollama:latest container_name: ollama restart: unless-stopped volumes: - ./ollama/ollama:/root/.ollama tty: true ports: - 11434:11434 networks: - ollama-docker # deploy: # resources: # reservations: # devices: # - driver: nvidia # count: 1 # capabilities: [gpu] ollama-webui: image: ghcr.io/ollama-webui/ollama-webui:main container_name: ollama-webui restart: unless-stopped volumes: - ./ollama/ollama-webui:/app/backend/data ports: - 8080:8080 environment: - &amp;#34;/ollama/api=http://ollama:11434/api&amp;#34; extra_hosts: - host.docker.internal:host-gateway networks: - ollama-docker networks: ollama-docker: Setup # Run docker-compose docker-compose up -d # Pull model(https://ollama.</description></item></channel></rss>